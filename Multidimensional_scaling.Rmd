---
title: "Multidimensional Scaling"
author: "Ananyo Dey Shreya Chatterjee Kaustav Paul"
date: "`r Sys.Date()`"
geometry: margin = 1.5 in
output: pdf_document
fontsize: 12pt
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Introduction

**Multidimensional scaling (MDS)** is a technique that translates
similarities (or differences) between pairs of items into distances
within a lower-dimensional space. The data, for example, may be
correlations among intelligence tests, and the MDS representation is a
plane that shows the tests as points that are closer together the more
positively the tests are correlated. By visually depicting these
correlations, MDS allows analysts to visually inspect the data, often
revealing patterns not apparent in raw numerical arrays. Additionally,
MDS can be utilized to construct models for assessing dissimilarities.
For example, given two objects of interest, one may explain their
perceived dissimilarity as the result of a mental arithmetic that mimics
the distance formula.According to this model, the mind generates an
impression of dissimilarity by adding up the perceived differences of
the two objects over their properties.

The basic purpose of our study is to demonstrate how multidimensional
scaling can help systematize data in areas where organizing concepts and
underlying distributions are not well developed. Multidimensional
scaling is simply a useful mathematical tool that enables us to
represent the similarities (or dissimilarities) of objects spatially as
in a map.

## Why MDS?

One of the principal advantages of MDS is its **ease of
interpretation**. MDS translates the given data as points in low
dimensional spaces keeping similarities and dissimilarities among the
points in means of distances of the corresponding plotted points.
Graphical representations always create clearer impression about the
data structure than mere numerical summaries. Let us consider an
example. In Table 1, we are provided with the mutual distances of ten
Spanish cities. Suppose a random person has no access to the
geographical map of the concerned region but he is provided with
distance matrix, it will be easier for him to get an idea of the
distances between the cities have the cities been represented as points
with distances between the plotted points being similar to the original
geographical distances as in Figure 2 . Hence MDS comes into great help
in such a situation.

Let us consider another scenario. It may happen that instead of values
of the concerned variables we are provided with some measure of
similarity and dissimilarity between the objects. For instance, a tea
taster upon tasting a number of tea flavors rates how much the taste of
one flavor differs from another. In this case also we can use MDS
proficiently.

Moreover MDS represents the objects as points in low dimensional spaces
(mostly two or three dimensional spaces are taken in to consideration).
Hence it can be used as a tool of dimension reduction.

MDS can also be used to find clusters among points. Suppose the data
points can be classified into a number of groups, in that case the
points coming from the same group can be expected to be similar to each
other. So MDS will reflect clusters among the plotted points. Such
examples will be considered later in the study.

```{r tables, echo=F, fig.cap="Matrix representation of the distance between eight Australian cities"}
dist.au <- read.csv("http://rosetta.reltech.org/TC/v15/Mapping/data/dist-Aus.csv")
dist.au <- dist.au[,-1]
row.names(dist.au) <- c("Adelaide", "Alice Springs", "Brisbane", "Darwin", "Hobart", "Melbourne", "Perth", "Sydney")
colnames(dist.au) <- c("Adelaide", "Alice Springs", "Brisbane", "Darwin", "Hobart", "Melbourne", "Perth", "Sydney")
knitr::kable(dist.au, caption = "Matrix representation of the distance between eight Australian cities")

```

```{r,echo=FALSE, fig.cap= "Representation of the cities as points on two dimensional space after MDS"}
fit <- cmdscale(dist.au, eig = TRUE, k = 2)
x <- fit$points[, 1]
y <- fit$points[, 2]
plot(x, y, pch = 19, xlim = range(x) + c(0, 600))
city.names <- c("Adelaide", "Alice Springs", "Brisbane", "Darwin", "Hobart", 
    "Melbourne", "Perth", "Sydney")
text(x, y, pos = 4, labels = city.names)
```

## The Basic Setup

Suppose a set of $n$ objects is under consideration and between each
pair objects $(r,s)$ there is a measurement $\delta_{rs}$ of the
"dissimilarity" between objects. For example the set of objects might be
ten bottles of whisky, each from a different distillery. The
dissimilarity $\delta_{rs}$ might be an integer score between zero and
ten given to the comparison of the $r^{th}$ and the $s^{th}$ whiskys by
an expert judge of malt whisky. The judge would be given a tot from the
$r^{th}$ bottle and one from the $s^{th}$ and then score the comparison:
0-the whiskies are so alike she/he cannot tell the difference, to 10-the
whiskies are totally different. The judge is presented with all
forty-five possible pairs of whiskies, and after a pleasant day's work,
provides the data analyst with a total set of dissimilarities
$\{\delta_{rs}\}$.

We may be provided with a different type of situation where we have
values corresponding to $p$ variables corresponding to the $n$ objects.
We represent the information in vector representation, as
\textbf{$x_1$}, \textbf{$x_2$}, ..., \textbf{$x_n$} where each
\textbf{$x_1$}, \textbf{$x_2$}, ..., \textbf{$x_n$} are $p$-vectors. In
this case we have to obtain the mutual distance (dissimilarity) between
the objects.

A narrow definition of multidimensional scaling (often abbreviated to
MDS) is the search for a low dimensional space, usually Euclidean, in
which points in the space represent the objects (whiskies), one point
representing one object, and such that the distances between the points
in the space, $\{\delta_{rs}\}$, match, as well as possible, the
original dissimilarities $\{\delta_{rs}\}$. The techniques used for the
search for the space and the associated configuration of points form
metric and nonmetric multidimensional scaling.

## Some Definitions

We describe preliminary material on proximity and metrics.

\textit{Proximity} literally means nearness in space, time or in some
other way. The "nearness" of objects, individuals, stimuli needs
definition and measurement prior to statistical analysis. In some
situations, this is straightforward, but in others, difficult and
controversial. Measures of proximity are of two types: similarity and
dissimilarity with the obvious interpretation of measuring how similar
or dissimilar objects are to each other. In this section, we introduce
some of the terminology and define some of the proximity measures
discussed in the following chapters.

\textbf{Definiton (Distance Matrix):} An $(n \times n)$ matrix
\textbf{D} is called a distance matrix if it is symmetric and
$$ d_{rr} = 0,\hspace{1mm} d_{rs} \geq 0, r\neq s. $$ The first property
above is called reflectivity, and the second property is called
non-negativity. Note that there is no need to satisfy the triangle
inequality.

\textbf{Definiton (Euclidean Distance):} The Euclidean distance between
two points $p = (p_1, ..., p_n)$ and $q = (q_1, ..., q_n)$ in $R^n$ is
given by the formula, $$ d(p,q)= \sqrt{\sum_{i = 1}^{n}(p_i - q_i)^2} $$

\textbf{Definition (Euclidean Distance Matrix):} A distance matrix
\textbf{D} is called Euclidean if there exists a configuration of points
in some Euclidean space whose interpoint distances are given by
\textbf{D}; that is, if for some $p$, there exists points
\textbf{$x_1$}, ..., \textbf{$x_n$} $\in R^p$ such that}
$$d_{rs}^2 = (x_r - x_s)^T(x_r - x_s)$$ where $^T$ denotes the transpose
of a matrix.

\textbf{Definition (Metric):} A function $d: X \times X \rightarrow R$
is called a metric if the following conditions are fulfilled for all
$x, y, z \in X:$

```{=tex}
\begin{itemize}
\item{(reflectivity) $d(x, x) = 0$}
\item{(positivity) $d(x, y) > 0$ for $x \neq y$}
\item{(symmetry) $d(x, y) = d(y, x)$}
\item{(triangle inequality) $d(x, y) \leq d(x, z) + d(z, y)$}
\end{itemize}
```
A metric space (X, d) is a set X equipped with a metric
$d: X \times X \rightarrow R$\
In some situations we start not with distances between $n$ objects, but
with similarities.

\textbf{Definition (Similarity Matrix):} An $(n \times n)$ matrix
\textbf{C} is called a similarity matrix if $c_{rs} = c_{sr}$ and if
$c_{rs} \leq c_{sr}$ for all $r, s$\
To use the techniques of the preceding method, it is necessary to
transform the similarities to distances. A useful transformation is the
following. The standard transformation from a similarity matrix
\textbf{C} to a distance matrix \textbf{D} is defined by
$$ d_{rs} = (c_{rr} - 2c_{rs} + c_{ss})^{\frac{1}{2}}$$ Hence,
\textbf{D} is a distance matrix.\
Similarities (dissimilarities) are constructed from a data matrix for
the objects. These are then called similarity (dissimilarity)
coefficients. Several authors for an example Cormack, Jardine and
Sibson, Anderberg, Sneath and Sokal, Jackson \textit{et al.}, Snijders
\textit{et al.} discuss various similarity and dissimilarity measures
together with their associated problems. The following synthesis of the
work of these authors attempts to outline the main ideas behind forming
dissimilarities from a data matrix. Let, \textbf{X} $= [x_{ri}]$ denoted
the data matrix obtained for $n$ objects on $p$ variables. The vector of
observations for the $r^{th}$ object is denoted by \textbf{$x_r$} =
[\textbf{$x_r^T$}].

```{=tex}
\begin{itemize}
\item{ \textit{Quantative Data :} The following table gives some of possible dissimilarity measures for quantitative data that are in particular, continuous, possibly discrete, but not binary.}
\end{itemize}
\begin{center}
\begin{tabular}{|c|c|}
  \hline
  Euclidean Distance & $\delta_{rs} = \{ \sum_i (x_{ri} - x_{si})^2\}^{\frac{1}{2}}$ \\
  \hline
  Mahalanobis Distance & $\delta_{rs} = $ {(\text{$x_r$} - \textbf{$x_s$})$^T \Sigma^{-1}${(\text{$x_r$} - \textbf{$x_s$})}}\\
  \hline
  Minkowski metric &  $\delta_{rs} = \{\sum_{i} w_i |x_{ri} - x_{si}|^{\lambda}\}^{\frac{1}{\lambda}} , \lambda \geq 0$\\
  \hline
  Bhattacharya distance & $\delta_{rs} = \{ \sum_i (x_{ri}^{\frac{1}{2}} - x_{si}^{\frac{1}{2}})^2\}^{\frac{1}{2}}$ \\
  \hline
  Bray-Curtis & $\delta_{rs} = \frac{1}{p}\frac{\sum_i |x_{ri} - x_{si}|}{\sum_i (x_{ri} + x_{si})}$\\
  \hline
\end{tabular}
\end{center}
```
```{=tex}
\begin{itemize}
\item{\textit{Binary Data :}}
\end{itemize}
```
```{=tex}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
        & & \textbf{Object s} & & \\
        & & $1$ & $0$ & \\
         & $1$ & $a$ & $b$ & $a+b$ \\
        \cline{2-5}
        \textbf{Object s} & $0$ & $c$ & $d$ & $c+d$ \\
        \hline
    \end{tabular}
\end{center}
```
The following table gives some of possible dissimilarity measures for
binary data\

```{=tex}
\begin{center}
\begin{tabular}{|c|c|}
  \hline
  Hamman & $s_{rs} = \frac{a-(b+c)+d}{a+b+c+d}$\\
  \hline
  Braun, Blanque & $s_{rs} = \frac{a}{max\{(a+b),(a+c)\}}$\\
  \hline
  Kulczynski & $s_{rs} = \frac{a}{(b+c)}$\\
  \hline
  Yule & $s_{rs} = \frac{ad - bc}{ad + bc}$\\
  \hline
  Russell, Rao & $s_{rs} = \frac{a}{(a+b+c+d)}$\\
  \hline
\end{tabular}
\end{center}
```
# Prime Goals

Suppose there are $n$ objects with dissimilarities $\{\delta_{rs}\}$.
Metric MDS attempts to find a set of points in a Euclidean space where
each point represents one of the objects and the distance between points
$\{\delta_{rs}\}$ are such that\
\begin{equation}
d_{rs} \approx f(\delta_{rs})
\end{equation} where $f$ is a continuous parametric monotonic function.
The function $f$ can either be the identity function or a function that
attempts to transform the dissimilarities to a distance-like form.
Mathematically, let the objects comprise a set $O$. Let the
dissimilarity, defined on $O \times O$, between objects $r$ and $s$ be
$\delta_{rs} (r, s \in O)$. Let $\phi$ be an arbitrary mapping from $O$
to $E$, where $E$ is usually a Euclidean space, but not necessarily so,
in which a set of points are to represent the objects. Thus let
$\phi(r) = x_r (r \in O, x_r \in E)$, and let $X = \{x_r : r \in O \}$,
the image set. Let the distance between the points $x_r$, $x_s$ in $X$
be given by $d_{rs}$. The aim is to find a mapping $\phi$, for which
$d_{rs}$. is approximately equal to $f(\delta_{rs})$ for all
$r, s \in O$. The two main metric MDS methods, classical scaling and
least squares scaling, will be considered here.

## Nature of the Solution:

The configuration produced by any MDS method is indeterminate with
respect to translation, rotation, and reflection. 1n the two-dimensional
case of road distances the whole configuration of points can be
"shifted" from one place in the plane to another and the whole
configuration can be "rotated" or "reflected".

In general, if $P_1, ..., P_n$ with coordinates $\mathbf{x_i'}$ =
$(x_{i1}, x_{i2}, ..., x_{ip})$, $i = 1, ..., n$, represents an MDS
solution in $p$ dimensions, then

$$\mathbf{y_i} = A\mathbf{x_i} + \mathbf{b}$$ is also a solution, where
$A$ is an orthogonal matrix and $\mathbf{b}$ is any vector. This is
because multiplying a matrix by an orthogonal matrix only leads to
rotation of the axes. Hence by the above matrix operation the solutions
are just rotated and shifted from one position to another on the two
dimensional plane which does not affect the feasibility to be a solution
in an MDS problem.\

## Types of Solution:

Methods of solution using only tbe. rank order of the distances
$$d_{r_1s_1} < d_{r_2s_2} < ... < d_{r_ms_m}, \quad \quad  m = \frac{1}{2}n(n - 1),$$
where $(r_1,s_1), ..., (r_m,s_m)$ denote all pairs of subscripts of $r$
and $s$, $r < s$, are titled as *non-metric methods of multidimensional
scaling*.

The rank orders are invariant under monotone increasing transformations
$f$ of the $d_{rs}$, i.e. $$
d_{r_1s_1} < d_{r_2s_2} < ... \Leftrightarrow f(d_{r_1s_1}) < f(d_{r_2s_2}) < ... $$

Therefore the configurations which arise from non-metric scaling are
indeterminate not only with respect to translation, rotation, and
reflection, but also with respect to uniform expansion or contraction.

Solutions which try to obtain $P$, directly from the given distances are
called *metric methods*. These methods derive $P$, such that, in some
sense, the new distances $\hat{d}_{rs}$ between points $P_r$, and $P_s$,
are as close to the original $d_{rs}$ as possible.

One important use of MDS is seriation. The aim here is to order a set of
objects chronologically on the basis of dissimilarities or similarities
between them. Suppose the points in the MDS configuration in $k = 2$
dimensions lie nearly on a smooth curve. This property then suggests
that the differences in the data are in fact one-dimensional and the
ordering of the points along this curve can be used to seriate the data.

## Classical Scaling

Classical scaling originated in the 1930s when Young and Householder
(1938) showed how starting with a matrix of distances between points in
a Euclidean space, coordinates for the points can be found such that
distances preserved. Torgerson (1952) brought the subject to popularity
busing the technique for scaling.

Starting with a distance matrix D, the object of MDS is to find points
$P_1, P_2, ..., P_n$ in $k$ dimensions such that if $\hat{d_{rs}}$
denotes the Euclidean distance between $P_r$ and $P_s$, then
$\hat{\textbf{D}}$ is "similar" in some sense to \textbf{D}. The points
$P_r$ are unknown and usually the dimension $k$ is also unknown. In
practice, on usually limits the dimension to $k$ = 1, 2 or 3 in order to
facilitate the interpretation of the solution. The following theorem
enables us to tell whether \textbf{D} is Euclidean, and, if so, how to
find a corresponding configuration of points. For any distance matrix
\textbf{D}, let \begin{equation}
A = ((a_{rs})), \hspace{5mm} a_{rs} = -\frac{1}{2}d_{rs}^2
\end{equation} and set \textbf{B = HAH} where $H = I - \frac{1}{n}11^T$
is the $(n \times n)$ centering matrix.\

\textbf{Theorem}: Let \textbf{D} be a distance matrix and defined by
\textbf{B} by previous, then \textbf{D} is Euclidean
\textit{if and only if} \textbf{B} is p.s.d. In particular, the
following results hold:

```{=tex}
\begin{enumerate}
\item{If \textbf{D} is the matrix of Euclidean interpoint distances fo a configuration \textbf{Z} = (\textbf{$z_1$}, ..., \textbf{$z_n$})$^T$, then 
\[ \mathbf{b_{rs}} = (\mathbf{z_r} - \mathbf{\bar{z}})^T(\mathbf{z_s} - \mathbf{\bar{z}}), \quad \quad r,s = 1, ..., n \]

In matrix form it becomes \textbf{B = (HZ)(HZ)$^T$} so \textbf{B} $\geq 0$. Note that \textbf{B} can be interpreted as the "\textit{centred inner product matrix} for configuration of \textbf{Z}}


\item{Conversely, if \textbf{B} is p.s.d of rank p then a configuration corresponding to \textbf{B} can be constructed as follows, let $\lambda_1 > ... > \lambda_p$ denote the positive eigenvalues of \textbf{B} with corresponding eigenvectors $\mathbf{X} = (\mathbf{x_{(1)}}, ..., \mathbf{x_{(p)}})^T$ normalized by
\[\mathbf{x_{(i)}}^T\mathbf{x_{(i)}} = \lambda_i, \quad \quad i = 1, 2 ,... ,p\]

Then the points P, in $\mathbb{R}^p$ with coordinates $\mathbf{x_r} = (\mathbf{x_{(r1)}}, ..., \mathbf{x_{(rp)}})^T$ (so \textbf{$x_r$} is the $r^{th}$ row of \textbf{X}) have interpoint distances given by \textbf{D}. Further, this configuration has centre of gravity $\bar{x} = 0$, and \textbf{B} represents the inner product matrix for this configuration}
\end{enumerate}
```
  To be of practical use, a configuration of points needs to be found
for a set of dissimilarities $\{\delta_{rs}\}$ rather than simply for
true Euclidean distances between points $\{d_{rs}\}$.

Suppose dissimilarities $\{\delta_{rs}\}$ are used instead of distances
$d_{rs}$ to define matrix \textbf{A}, which is then doubly centred to
produce matrix \textbf{B} as just described. Then it is interesting to
ask under what circumstances \textbf{B} can give rise to a configuration
of points in Euclidean space, using the spectral decomposition, so that
the associated distances $\{\delta_{rs}\}$ are such that
$d_{rs} = \delta_{rs}$ for all $r, s$. The answer is that if \textbf{B}
is positive semi-definite pf rank $p$, then configuration in $p$
dimensional Euclidean space can be found.

The next question to be asked is how many dimensions are required in
general for the configuration of points produced from a positive
semi-definite matrix \textbf{B} of dissimilarities. It can be shown that
\textbf{B} has at least one zero eigenvalue, since
\textbf{B1 = HAH1 = 0}. Thus a configuration of points in an $n-1$
dimensional Euclidean space can always be found whose associated
distances are equal to the dissimilarities $\{\delta_{rs}\}$. Choose the
configuration in $R^k$ whose coordinates are determined by the first $k$
eigenvectors of \textbf{B}. If the first $k$ eigenvalues of \textbf{B}
are "large" and positive, and hopefully, the interpoint distances of
this configuration will closely approximate \textbf{D}. This
configuration is called the
\textit{Classical solution to the MDS problem in $k$ dimensions}. It is
metric solution.

Another suitable question can be what will happen if \textbf{B} is not
positive semi-definite. Then a constant can be added to all the
dissimilarities (except the self-dissimilarities $\delta_{rr}$) which
will then make \textbf{B} positive semi-definite. Thus forming new
dissimilarities, $\{\delta_{rs}\prime\}$ as
$\delta_{rs}\prime = \delta_{rs} + c(1 - \delta_{rs})$, where $c$ is an
appropriate constant and $\delta_{rs}$ the Kronecker delta will make
\textbf{B} positive semi-definite. Once \textbf{B} has been made
positive semi-definite, a Euclidean space can be found as before where
distances $d_{rs}$ are exactly equal to dissimilarities
$\delta_{rs}\prime$.

## A Practical Algoritm to Classical MDS

Suppose we are given a distance matrix $D$ which we hope can
approximately represent the inter-point distances of a configuration in
a Euclidean space of low dimension $k$ (usually $k = l, 2,$ or $3$). The
matrix D mayor may not be Euclidean; however, even if $D$ is Euclidean,
the dimension of the space in which it can be represented will usually
be too large to be of practical interest. Classical MDS uses the
following algorithm to determine the required solution:

```{=tex}
\begin{enumerate}
  \item{From $D$ construct the matrix $A = -\frac{1}{2}d_{rs}^2$}
  \item{Obtain the matrix $B$ with elements $b_{rs} = a_{rs} - \bar{a}_{i\cdot} - \bar{a}_{\cdot s} + \bar{a}_{\cdot \cdot}$}
  \item{Find the $k$ largest eigenvalues $\lambda_1 > . . . > \lambda_k$ of $B$($k$ chosen ahead of time), with corresponding eigenvectors $X = (\mathbf{x_{(i)}}, ..., \mathbf{x_{(k)}})$} which are normalized by $\mathbf{x'_{(i)}}\mathbf{x_{(i)}} = \lambda_i, i = 1,..., k$. (We are supposing here that the first $k$ eigen values are all positive.)
  \item{The required coordinates of the points $P_r$ are $\mathbf{x_r} = (x_{r1}, ... , x_{rp})',$ $r = 1,...,k,$ the rows of $X$.  
r= 1. . . . , k, the rows of X.}
\end{enumerate}
```
We will now apply this algorithm over a data set.

## An Example

We will now apply Classical MDS over Fisher's \texttt{iris} data set.

The data set contains measurements in centimeters of the variables sepal
length and width and petal length and width, respectively, for 50
flowers from each of 3 species of iris. The species are *Iris setosa*,
*versicolor*, and *virginica*. A glimpse of the data set is given below:

```{r table, echo=F}
library(MASS)
data(iris)
knitr::kable(head(iris))

```

We have used the \texttt{cmdscale} function in the \texttt{stats}
package in \texttt{R} to perform the Classical MDS algorithm over the
data points. The function returns the optimal points to be plotted on a
$k$ dimensional space. We have considered $k = 2$ in this case. We have
plotted the points in Figure 2.

```{r, echo=F,  message=FALSE, warning=FALSE, fig.cap= "Classical MDS on iris data for $k=2$", fig.pos="H"}
library(ggplot2)
library(plotly)
mds_iris <- cbind(as.data.frame(cmdscale(dist(iris[,1:4]))), iris[,5])
colnames(mds_iris) <- c("x", "y", "variety")

fig <- ggplot(data = mds_iris, aes(x = x, y = y, color = variety)) + geom_point()
fig
```

From the plot we can get a clear idea about the dissimilarities in the
features corresponding to different flowers. The flowers belonging to
the same category are put close to each other indicating that the extent
of dissimilarity between two flowers of same category is lesser than
those between two belonging to different categories. Thus in this case
MDS helps us in finding clusters among the data points.

## Similarities

We may also be provided with similarities among the data points instead
of distances (or dissimilarities). We have already defined a similarity
matrix in a previous section. To use the techniques of the preceding
sections, it is necessary to transform the similarities to distances. We
have previously discussed a transformation to convert a similarity
matrix to a distance matrix as follows,
$$d_{rs} = (c_{rr} - 2c_{rs} + c_{ss})^{\frac{1}{2}}$$ In fact the
transformed distance is an Euclidean distance matrix.

\textbf{Theorem:} If $C \geq O$, then the distance matrix $D$ defined by
the above standard transformation is Euclidean, with centered inner
product matrix $B = HCH$.

### Classical Scaling and Principal Components

Suppose $X$ is a data matrix of dimension $n \times p$. The sample
covariance matrix obtained from $X$ is $S = (n-1)^{-1} X^TX$, where it
is assumed that the data have been mean corrected. Principal components
are obtained by finding eigen values $\{\lambda_i: i = 1, ..., p\}$ and
right eigen vectors $\{\mathbf{e_i} : i = 1, ..., p\}$ of $S$, and then
the $i$th principal component is given by
$u_i = \mathbf{e_i}^T\mathbf{x}, (i = 1, ..., p)$

Suppose on the other hand Euclidean distance is used on the data matrix
$X$ to define dissimilarities among the $n$ dissimilarities among the
$n$ individuals or objects. The dissimilarities will be given by
$$\delta^2_{rs} = (\mathbf{x_r} - \mathbf{x_s})^T(\mathbf{x_r} - \mathbf{x_s})$$

and hence when these dissimilarities are subjected to classical scaling,
$b_{rs} = \mathbf{x_r}^T\mathbf{x_s}$ and $B = XX^T$.

As before, let the eigenvalues of $B$ be $l_i (i = 1, ..., p)$ with
associated eigen vectors $\mathbf{v_i} (i = 1,...,n)$.

It is a well known result that the eigen values of $XX^T$ are the same
as those for $X^TX$, together with an extra $n-p$ zero eigenvalues. This
is easily shown. Let $\mathbf{v_i}$ be an eigen vector of $XX^T$
associated with a non-zero eigen value, and so

$$XX^T\mathbf{v_i} = l_i\mathbf{v_i}$$

Premultiplying by $X^T$,

$$(X^TX)(X^T\mathbf{v_i}) = l_i(X^T\mathbf{v_i})$$

But

$$X^TX\mathbf{e_i} = \lambda_ie_i$$

and hence $\lambda_i = l_i$ and the eigenvectors are related by
$e_i = X^T\mathbf{v_i}$. Thus there is duality between a principal
components analysis and a Classical MDS where dissimilarities are given
by Euclidean distance. In fact, the coordinates obtained in $p'$
dimensions for the $n$ objects by CMDS are simply the component scores
for the $n$ objects on the first $p'$ principal components. Now
$\mathbf{e_i}^t\mathbf{e_i} = \mathbf{v_i}^TXX^T\mathbf{v_i} = l_i$.
Normalizing $\mathbf{e_i}$, the first $p'$ component scores are given by

```{=tex}
\begin{align*} 
  X[l_1^{-1}e_1, l_2^{-1}e_2, ..., l_{p'}^{-1}e_{p'}] &= X[l_1^{-1}X^T\mathbf{v_1}, ..., l_{p'}^{-1}X^T\mathbf{v_{p'}}] \\
                                                     &= [l_1^{-\frac{1}{2}}XX^T\mathbf{v_1}, ..., l_{p'}^{-1\frac{1}{2}}XX^T\mathbf{v_{p'}}] \\
                                                     &= [l_1^{-\frac{1}{2}}\mathbf{v_1}, ..., l_{p'}^{-1\frac{1}{2}}\mathbf{v_{p'}}] 
\end{align*}
```
which are the coordinates obtained from CMDS in $p'$ dimensions.

We have also computed the first two principal components for
\texttt{iris} data. Figure 3 shows that the principal components
coincide with the classical solution of MDS.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#ultimate_PCA
library(dplyr)
library(ggplot2)
library(gridExtra)
data(iris) 
mydata <- select(iris,c(1,2,3,4))
cor(mydata)
mean(cor(mydata))
#pricipal component
PCA <- princomp(mydata)
PCA$loadings
PC = PCA$scores
View(PC)
cor(PC)
Species <- iris$Species
PComp <- cbind(as.data.frame(PC), Species)

PComp
pca_plot <- ggplot(data = PComp) + geom_point(aes(x = Comp.1, y = Comp.2, col = Species)) + labs(title = "First two principal components obtained from iris data")
pca_plot



# Perform MDS analysis 
mds_iris <- cbind(as.data.frame(cmdscale(dist(iris[,1:4]))), iris[,5])
colnames(mds_iris) <- c("x", "y", "Species")

mds_plot <- ggplot(data = mds_iris, aes(x = x, y = y, color = Species)) + geom_point() + labs(title = "Solution obtained by classical MDS for iris data")

gridExtra::grid.arrange(pca_plot, mds_plot, nrow = 2) 

```

```{r echo = F, message=FALSE, warning=FALSE, fig.cap="Comparison of PCA and classical MDS solution for iris data",}
combined_plot <- grid.arrange(pca_plot, mds_plot, nrow = 2)
```

## Optimal Properties of the Classical Solution and Goodness of Fit

Given a distance matrix $D$, the object of MDS is to find a
configuration $\hat{X}$ in a low-dimensional Euclidean space
$\mathbb{R}^p$ whose interpoint distances,
$\hat{d}^2_{rs} = (\hat{\mathbf{x}}_r - \hat{\mathbf{x}}_s)'(\hat{\mathbf{x}}_r - \hat{\mathbf{x}}_s)$
say, closely match $D$ . The "hat" will be used in this section to
indicate that the inter-point distances $\hat{D}$ for the configuration
$\hat{X}$ are "fitted" to the original distances $D$. Similarly, let
$\hat{B}$ denote the fitted centered inner product matrix.

Now let $X$ be a configuration in $\mathbb{R}^p$ and let $L=(L_1, L_2)$
be a $(p \times p)$ orthogonal matrix where $L_1$ is $(p \times k)$.
Then $XL$ represents a projection of the configuration $X$ onto the
subspace of $\mathbb{R}^p$ spanned by the columns of $L_1$. We can think
of $\hat{X} = XL$ as a "fitted" configuration in $k$ dimensions. Since
$L$ is orthogonal, the distances between the rows of $X$ are the same as
the distances between the rows of $XL$,

$$\hat{d}^2_{rs} = \sum_{i = 1}^{p} (x_{ri} - x_{si})^2 = \sum_{i = 1}^{p} (\mathbf{x'_rl_{(i)}} - \mathbf{x'_sl_{(i)}})^2.$$
If we denote teh distances between the row of $XL_1$ by $\hat{D}$, then

$$\hat{d^2_{rs}} =  \sum_{i = 1}^{k} (\mathbf{x'_rl_{(i)}} - \mathbf{x'_sl_{(i)}})^2$$
Thus $\hat{d_{rs}} \leq d_{rs}$; that is, projecting a configuration
reduces the inter-point distances. Hence, a measure of the discrepancy
between the original configuration $X$ and the projected configuration
$\hat{X}$ is given by

$$\phi = \sum_{r,s = 1}^nd^2_{rs} - \hat{d^2_{rs}}$$

Then the classical solution to the MDS problem in $k$ dimensions has the
following optimal property:

\textbf{Theorem:}: *Let* $D$ be a Euclidean distance matrix
corresponding to a configuration $X$ in $\mathbb{R}^p$, and fix
$k (1 \leq k < p)$. Then amongst all projections $XL_1$, of $X$ onto
k-dimensional subspaces of $\mathbb{R}^p$, the quantity $\phi$ is
minimized when $X$ is projected onto its principal coordinates in $k$
dimensions.

We have already seen that solutions obtained by classical MDS coincide
with the principal components. So the solution obtained by classical MDS
provides the least measure of discrepancies.

When $D$ is not necessarily Euclidean, it is more convenient to work
with the matrix $B = HAH$. If $\hat{X}$ is a fitted configuration with
centered inner product matrix $\hat{B}$, then a measure of discrepancy
between $B$ and $\hat{B}$ is given by;

$$\psi = \sum_{r,s = 1}^n(b_{rs} - \hat{b}_{rs})^2 = tr(B - \hat{B})$$
For this measure also, we can prove that the classical solution to the
MDS problem is optimal. Let us look at the following theorem.

\textbf{Theorem:} *If* $D$ is a distance matrix (not necessarily
Euclidean), the for fixed $k$, $\psi$ defined earlier is minimized over
all configurations $\hat{X}$ in $k$ dimensions when $\hat{X}$ is the
classical solution to the MDS problem.

The above two theorems suggest possible *agreement measures* for the
"proportion of a distance matrix $D$ explained" by the $k$-dimensional
classical MDS solution. Supposing $\lambda_k > 0$
($\lambda_1 \geq ... \lambda_n$ denote the eigen values of $B$), these
measures are

$$\alpha_{1,k} = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^n |\lambda_i|} \times 100 \%,$$

and

$$\alpha_{2,k} = \frac{\sum_{i=1}^k \lambda_i^2}{\sum_{i=1}^n \lambda_i^2} \times 100 \%$$
The absolute values of the eigenvalues are considered in the denominator
of $\alpha_{1,k}$ because some of the smaller eigen values might be
negative.

#### Example

We have calculated the two agreement measures discussed above for the
\texttt{iris} data.

$$\alpha_{1,2} = 97.7685 \%; \quad \alpha_{2,2} = 99.9627 \%$$ So we can
interpret our obtained results as "*the classical two dimensional
classical MDS solution explains a huge proportion of the original
dissimilarity matrix*".

In the next section we will discuss about Non-Metric methods of
Multidimensional Scaling.

\newpage

# \textbf{Nonmetric Multidimensional Scaling}

### Introduction

This section presents the underlying theory of non-metric
multidimensional scaling developed in the 1960s.

Suppose there are $n$ objects with dissimilarities $\{\delta_{rs}\}$.
The procedure is to find a configuration of $n$ points in a space, which
is usually chosen to be Euclidean, so that each object is represented by
a point in the space. A configuration is sought so that distances
between pairs of points $\{d_{rs}\}$ in the space match "as well as
possible" the original dissimilarities $\{\delta_{rs}\}$.

Mathematically, let the objects comprise a set $O$. Let the
dissimilarity, defined on $O \times O$, between objects $r$ and $s$ be
$\delta_{rs}$ $(r, s \in O)$. Let $\phi$ be an arbitrary mapping from
$O$ onto a set of points $X$, where $X$ is a subset of the space which
is being used to represent the objects. Let the distance between points
$x_r, x_s$ in $X$ be given by the real-valued function $d_{x_rx_s}$.
Then a disparity, $\hat{d}$, is defined on $O \times O$, which is a
measure of how well the distance $d_{\phi(r)\phi(s)}$ matches
dissimilarity $\delta_{rs}$. The aim is to find a mapping $\phi$, for
which $d_{\phi(r)\phi(s)}$ is approximately equal to $\hat{d}_{rs}$ and
is usually found by means of some loss function. The points in $X$
together with their associated distances will be referred to as a
configuration of points.

The choices are already discussed in previous section, and it is assumed
that dissimilarities $\{\delta_{rs}\}$ have been calculated for the set
of objects. The set $X$ is often taken as $\mathbb{R}^2$ and $d$ as
Euclidean distance, although others are sometimes used, for example
$\mathbb{R}^3$, and the Minkowski metric. Once these are chosen,
together with the method for calculating disparities, the nonmetric
multidimensional scaling problem becomes one of finding an appropriate
algorithm for minimizing a loss function.

One choice of $d_{rs}$ can be of the form,

$$d_{rs} = \delta_{rs} + e_{rs}$$

Here the $e_{rs}$ represent errors of measurement plus distortion errors
arising because the distances do not exactly correspond to a
configuration in $\mathbb{R}^k$. \newpage

However, in some situations it is more realistic to hypothesize a less
rigid relationship between $d_{rs}$ and $\delta_{rs}$; namely, suppose

$$d_{rs} = f(\delta_{rs} + e_{rs})$$ where $f$ is an unknown monotone
increasing function. For this "model", the only information we can use
to reconstruct the $\delta_{rs}$ is the rank order of the $d_{rs}$.

In this non-metric approach $D$ is not thought of as a "distance" matrix
but as a "dissimilarity" matrix. In fact the non-metric approach is
often most appropriate when the data is presented as a similarity
matrix. For in this situation the transformation from similarities to
distances is somewhat arbitrary and perhaps the strongest statement one
should make is that greater similarity implies less dissimilarity.

An algorithm to construct a configuration based on the rank order
information has been developed by Shepard and Kruskal. We will now
discus the algorithm.

## Shepard - Kruskal Algorithm

```{=tex}
\begin{enumerate}
  \item{Given a dissimilarity matrix $D$ , order the off-diagonal elements so
that

$$d_{r_1s_1} \leq ... \leq d_{r_ms_m}, \quad m = \frac{1}{2}n(n-1),$$

where $(r_1,s_1), ..., (r_m, s_m)$ denote all pairs of unequal subscripts,$r_i < s_i$. Say that numbers $d^*_{rs}$ are monotonically related to the $d_{rs}$ (and write $d^*_{rs} \buildrel mon \over \sim d_{rs}$) if 

$$d_{rs} < d_{uv} \implies d^*_{rs} \leq d^*_{uv} \quad \text{for all} \quad r<s, u<v  \quad \quad \quad ...(*)$$}

  \item{Let $\hat{X}(n \times k)$ be a configuration in
$\mathbb{R}^k$ with interpoint distances $\hat{d}_{rs}$. Define the (squared) stress of $\hat{X}$ by

$$S^2(\hat{X}) = \text{min} \frac{\sum_{r<s}(d^*_{rs} - \hat{d}_{rs})^2}{\sum_{r<s} \hat{d}^2_{rs}}, $$
where the minimum is taken over $d^*_{rs}$ such that $d^*_{rs} \buildrel mon \over \sim d_{rs}$. The $d^*_{rs}$ which minimizes $S^2(\hat{X})$ represent the least squares least squares monotone regression of $\hat{d}_{rs}$ on $d_{rs}$. Thus $S^2(\hat{X})$ represents the extent to which the rank order of the $\hat{d}_{rs}$ disagrees with the rank order of the $d_{rs}$. If the rank orders match exactly (which is very rare in practice), then $S(\hat{X}) = 0$. The presence of the denominator in $S^2(\hat{X})$ standardizes the stress and makes it invariant under transformations of the sort $\mathbf{y_r} = c\mathbf{x_r}, r = 1,...,n, c \ne 0.$ The stress is also invariant under transformations of the form $\mathbf{y_r} = A\mathbf{x_r} + \mathbf{b}$ when $A$ is orthogonal.
}
  \item{For each dimension $k$, the configuration which has the smallest stress is called the *best fitting configuration in $k$ dimension*. Let 
  $$S_k =  \min_{\hat{X}(n \times k)}  S(\hat{X})$$
  denote this minimal stress.
}

  \item{
  To choose the correct dimension, calculate $S_1, S_2, ...$ , until the value becomes low. Say, for example, $S_k$ is low for $k = k_0$. Since $S_k$ is a decreasing function of $k$, $k = k_0$ is the "right dimension". A rule of thumb is provided by Kruskal to judge the tolerability of $S_k$; $S_k \geq 20\%,$ poor; $S_k = 10\%$, fair; $S_k \leq 5\%,$ good; $S_k = 0$, perfect.
}

\end{enumerate}
```
#### Remarks:

```{=tex}
\begin{enumerate}
  \item{The "best configuration" starting from an arbitrary initial configuration can be obtained by using a computer routine developed by Kruskal which utilizes the method of steepest descent to find the local minimum. The initial configuration can be taken as the classical solution. Unfortunately, there is no way of distinguishing in practice between a local minimum and the global minimum.}
  \item{The Shepard-Kruskal solution is invariant under rotation, translation, and uniform expansion or contraction of the best-fining configuration.}
  \item{The Shepard-Kruskal solution is non-metric since it utilizes only the rank orders. However, we still need a sufficiently objective numerical measure of distance to determine the rank order of the $d_{rs}$.}
  \item{\textit{Similarities} The non-metric method works just as well with similarities as with dissimilarities. One simply changes the direction of the
inequalities.}
  \item{\textit{Missing values} The Shepard-Kruskal method is easily adapted to the situation where there are missing values. One simply omits the missing dissimilarities in the ordering and deletes the corresponding terms from the numerator and denominator of $S^2(\hat{X})$. As long as
not too many values are missing, the method still seems to work well.}
  \item{\textit{Treatment of ties} The constraint given by $(*)$ is called the primary treatment of lies (PTT). If $d_{rs} = d_{uv}$ then no constraint is made on $d^{*}_{rs}$ and $d^{*}_{uv}$. An alternative constraint, called the \textit{secondary treatment of ties} (STT) is given by
  $$d_{rs} \leq d_{uv} \implies d^*_{rs} \leq d^*_{uv},$$
which has the property that $d_{rs} = d_{uv} \implies d^*_{rs} = d^*_{uv}$.  
  }
  \item{\textit{Comparison of methods} The computation is simpler for the classical method than it is for the non-metric method. It is not known how robust the classical method is to monotone transformations of the distance function; however, both methods seem to give similar answers when applied to well-known examples in the field.}
\end{enumerate}
```


## Shepard Diagram:

In a general nonmetric scaling situation, using the Shepard–Kruskal
approach, if we have data $y_1, y_2, ... y_n$ and a model $f_i(\theta)$ with a number of free parameters $\theta$, to choose the parameters in such a way that the rank order of the model approximates the rank order of the data as well as possible we construct a loss function of the form,
$$\sigma(\theta, \hat{y})= \sum_{i=1}^{n}{w_i(\hat{y_i}-f_i(\theta))^2}$$
  where $w_i$ are known weights. We then minimize $\sigma$ over all $\hat{y}$ that are monotone with the data $y$ and over the parameters $\theta$.
After we have found the minimum, we plot the data $y$ on the
horizontal axis and $\hat{y}$ and the model values $f$ on the vertical axis m to draw the best-fitting monotone step function through the scatter plot. The scatter plot with $y$ and $f$ , and $\hat{y}$ drawn in, is called the \textit{Shepard diagram}. The vertical line of each point from the regression function $f$ gives the residual of this particular point. This gives us an overall picture of the scatter around the regression function including the possibility to detect outliers.
In our case, the loss function is, 
$$S(\hat{X}) = \frac{\sum_{r<s}(d^*_{rs} - \hat{d}_{rs})^2}{\sum_{r<s} \hat{d}^2_{rs}}$$
  So plotting $d_{rs}$ on the horizontal axis and $d^*_{rs}$ and $\hat{d}_{rs}$ on the vertical axis, we obtain Shepard Diagram. This shows how the dissimilarities and the approximated distances are related to each other.


## Goodness of Fit Measure: Procrustes Rotation 

We will now discuss about a goodness of fit measure used to compare two
configurations. Let $X$ be the $(n \times p)$ matrix of the coordinates
of $n$ points obtained from $D$ by one technique. Suppose that $V$ is
the $(n \times q)$ matrix of coordinates of another set of points
obtained by another technique, or using another measure of distance. Let
$q \leq p$. By adding columns of zeros to $Y$, we may also assume $Y$ to
be $(n \times p)$.

The measure of goodness of fit adopted is obtained by moving the points
$\mathbf{y_r}$ relative to the points $\mathbf{x_r}$ until the
"residual" sum of squares

$$\sum_{r = 1}^{n} (\mathbf{y_r} - \mathbf{x_r})'(\mathbf{y_r} - \mathbf{x_r})$$
is minimal. We can move $y_r$ relative to $x_r$ through rotation,
reflection and translation, i.e. by

$$A'\mathbf{y_r} + \mathbf{b}, \quad \quad r = 1,..., n, \quad \quad \quad ... (**)$$

where $A'$ is a $(p \times p )$ orthogonal matrix. Hence, we wish to
solve

$$R^2 = \min_{A,\mathbf{b}} \sum_{r = 1}^{n} (\mathbf{x_r} - A'\mathbf{y_r} - \mathbf{b})'(\mathbf{x_r} - A'\mathbf{y_r} - \mathbf{b})$$

for $A$ and $\mathbf{b}$. Note that $A$ and $b$ are found by least
squares. Their values are given in the following theorem. But before
that we state the *singular value theorem*.

\textbf{Theorem:} (Singular value decomposition theorem)
\textit{If $A$ is an
$(n x\times p)$ matrix of rank $r$, then $A$ can be written as}

$$A = ULV'$$

\textit{where $U(n \times r)$ and $V(p \times r)$ are orthogonal matrices $(U'U = V'V = I_r)$ and $L$ is a diagonal matrix with positive elements.}

\textbf{Theorem:} \textit{Let $X(n \times p)$ and
$Y(n \times p)$ be two configurations of $n$
points, for convenience centered at the origin, so $\bar{x} = \bar{y} = 0.$ Let $Z = Y'X$ and using the singular value decomposition theorem, write }

$$Z = V \Gamma U'. \quad \quad \quad ...(1)$$

\textit{where $\mathbf{V}$ and $\mathbf{U}$ are orthogonal $(p \times p)$ matrices and $\mathbf{\Gamma}$ is a diagonal matrix of non-negative elements. Then the minimizing values of $A$ and $\mathbf{b}$ for $R^2$ defined earlier are given by }

$$\mathbf{\hat{b}} = \mathbf{0}, \quad \mathbf{\hat{A}} = \mathbf{VU}', $$

\textit{and further}

$$R^2 = tr(\mathbf{XX}') + tr(\mathbf{YY}') - 2 tr(\mathbf{\Gamma}).\quad \quad \quad ...(2)$$

We have assumed that the column means of $\mathbf{X}$ and $\mathbf{Y}$ are zero. Then the
"best" rotation of $\mathbf{Y}$ relative to $\mathbf{X}$ is $Y\hat{A}$, where $\mathbf{\hat{A}}$ is
as defined in the previous theorem, and $\mathbf{\hat{A}}$ is called the
*Procrustes rotation* of $\mathbf{Y}$ relative to $\mathbf{X}$.

Using (1), $$\mathbf{X'YY'X} = \mathbf{Z'Z} = \mathbf{U \Gamma}^2 \mathbf{U}'$$ Thus we can write,
$$R^2 = tr(\mathbf{XX'}) + tr(\mathbf{YY'}) - 2 tr[(\mathbf{X'YY'X})^{1/2}]$$ So $R^2$ is zero if
and only if $Y$ can be rotated to $X$ exactly.

If the scales of $X$ and $Y$ are different, then the transformation
$(**)$ should be of form,
$$c\mathbf{A}'\mathbf{y_r} + \mathbf{b}, \quad \quad r = 1,..., n,$$ where c>0.

The estimate of c is, $$\hat{c}=tr(\mathbf{\Gamma})/tr(\mathbf{YY}')$$ and the other
estimates remain as before. This transformation is called the
*Procrustes rotation with scaling* of $\mathbf{Y}$ relative to $\mathbf{X}$ and the new
minimum residual sum of squares is given by,
$$R^2 = tr(\mathbf{XX}') + \hat{c}^2 tr(\mathbf{YY}') - 2\hat{c} tr[(\mathbf{X'YY'X})^{1/2}].$$
This procedure is not symmetrical with respect to $\mathbf{X}$ and
$\mathbf{Y}$. Symmetry can be obtained by selecting scaling so that
$$tr(\mathbf{XX}')=tr(\mathbf{YY}')$$\


